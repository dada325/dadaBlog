<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>BiomedParse Review | Math, Bio, Programming</title><meta name=keywords content="LLM"><meta name=description content="BiomedParse
1. Model Selection and Architecture
BiomedParse is a transformer-based language model specifically designed for parsing biomedical text into structured representations. The key architectural choices are:


Using a pre-trained BERT (Bidirectional Encoder Representations from Transformers) model as the backbone, which has shown strong performance on various natural language processing tasks. BERT is a deep bidirectional transformer that learns contextual word representations by jointly conditioning on both left and right context."><meta name=author content="dada"><link rel=canonical href=https://dada325.github.io/dadaBlog/posts/bioparse-review/><meta name=google-site-verification content="XYZabc"><meta name=yandex-verification content="XYZabc"><meta name=msvalidate.01 content="XYZabc"><link crossorigin=anonymous href=/dadaBlog/assets/css/stylesheet.8af8b13484f23a6472723a6ab042dc68c65bd12edf4092955621bd7e3eb4e84c.css integrity="sha256-ivixNITyOmRycjpqsELcaMZb0S7fQJKVViG9fj606Ew=" rel="preload stylesheet" as=style><link rel=icon href=https://dada325.github.io/dadaBlog/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://dada325.github.io/dadaBlog/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://dada325.github.io/dadaBlog/favicon-32x32.png><link rel=apple-touch-icon href=https://dada325.github.io/dadaBlog/apple-touch-icon.png><link rel=mask-icon href=https://dada325.github.io/dadaBlog/%3Clink%20/%20abs%20url%3E><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=https://dada325.github.io/dadaBlog/posts/bioparse-review/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><meta property="og:title" content="BiomedParse Review"><meta property="og:description" content="BiomedParse
1. Model Selection and Architecture
BiomedParse is a transformer-based language model specifically designed for parsing biomedical text into structured representations. The key architectural choices are:


Using a pre-trained BERT (Bidirectional Encoder Representations from Transformers) model as the backbone, which has shown strong performance on various natural language processing tasks. BERT is a deep bidirectional transformer that learns contextual word representations by jointly conditioning on both left and right context."><meta property="og:type" content="article"><meta property="og:url" content="https://dada325.github.io/dadaBlog/posts/bioparse-review/"><meta property="og:image" content="https://dada325.github.io/dadaBlog/%3Cimage%20path/url%3E"><meta property="article:section" content="posts"><meta property="article:published_time" content="2024-06-06T12:54:20+08:00"><meta property="article:modified_time" content="2024-06-06T12:54:20+08:00"><meta property="og:site_name" content="My Awesome Blog"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://dada325.github.io/dadaBlog/%3Cimage%20path/url%3E"><meta name=twitter:title content="BiomedParse Review"><meta name=twitter:description content="BiomedParse
1. Model Selection and Architecture
BiomedParse is a transformer-based language model specifically designed for parsing biomedical text into structured representations. The key architectural choices are:


Using a pre-trained BERT (Bidirectional Encoder Representations from Transformers) model as the backbone, which has shown strong performance on various natural language processing tasks. BERT is a deep bidirectional transformer that learns contextual word representations by jointly conditioning on both left and right context."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://dada325.github.io/dadaBlog/posts/"},{"@type":"ListItem","position":2,"name":"BiomedParse Review","item":"https://dada325.github.io/dadaBlog/posts/bioparse-review/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"BiomedParse Review","name":"BiomedParse Review","description":"BiomedParse 1. Model Selection and Architecture BiomedParse is a transformer-based language model specifically designed for parsing biomedical text into structured representations. The key architectural choices are:\nUsing a pre-trained BERT (Bidirectional Encoder Representations from Transformers) model as the backbone, which has shown strong performance on various natural language processing tasks. BERT is a deep bidirectional transformer that learns contextual word representations by jointly conditioning on both left and right context.\n","keywords":["LLM"],"articleBody":"BiomedParse 1. Model Selection and Architecture BiomedParse is a transformer-based language model specifically designed for parsing biomedical text into structured representations. The key architectural choices are:\nUsing a pre-trained BERT (Bidirectional Encoder Representations from Transformers) model as the backbone, which has shown strong performance on various natural language processing tasks. BERT is a deep bidirectional transformer that learns contextual word representations by jointly conditioning on both left and right context.\nFine-tuning the pre-trained BERT model on a large corpus of biomedical text (PubMed abstracts and PMC full-text articles) to adapt it to the biomedical domain. This domain-specific pre-training allows the model to learn the unique vocabulary, grammar, and semantic relationships in biomedical literature.\nAdding a token classification head on top of the BERT encoder to predict the entity labels (e.g., gene, disease, drug) for each input token. The token classification head is a linear layer followed by a softmax activation, which outputs the probability distribution over the entity labels.\nAdding a relation classification head to predict the relations (e.g., gene-disease association, drug-drug interaction) between pairs of entities. The relation classification head takes the concatenated entity representations as input and outputs the probability distribution over the relation types.\nJointly training the entity and relation classification heads using a multi-task learning objective, which combines the cross-entropy losses for both tasks. This joint training allows the model to learn the interactions between entities and relations and improve the overall parsing performance.\nThe transformer architecture allows BiomedParse to capture long-range dependencies and complex semantic relationships in biomedical text, while the domain-specific pre-training and multi-task learning enable it to adapt to the unique characteristics of biomedical literature.\n2. Data Handling and Preprocessing BiomedParse was trained on a large corpus of biomedical text, including:\n14 million PubMed abstracts (3.2 billion words) 2.7 million PMC full-text articles (7.5 billion words) The key preprocessing steps were:\nTokenizing the text into subword units using the WordPiece tokenizer, which splits words into smaller units based on their frequency in the corpus. This allows the model to handle out-of-vocabulary words and capture morphological patterns.\nRepresenting each input sequence as a sequence of token IDs, along with special tokens like [CLS] (classification token) and [SEP] (separator token) to demarcate the sequence boundaries.\nCreating entity labels for each token using the BIO (Beginning-Inside-Outside) tagging scheme, where ‘B’ denotes the beginning of an entity, ‘I’ denotes the inside of an entity, and ‘O’ denotes outside of an entity. The entity labels were obtained from existing biomedical named entity recognition datasets.\nCreating relation labels for each pair of entities in the input sequence, based on existing biomedical relation extraction datasets. The relation labels were represented as a binary matrix, where each cell indicates the presence or absence of a relation between two entities.\nThe preprocessed data was split into training, validation, and test sets for model development and evaluation. No data augmentation was used, as the large size of the training corpus was deemed sufficient for learning robust representations.\n3. Training Process BiomedParse was trained using a two-stage process: pre-training on the biomedical corpus, followed by fine-tuning on the entity and relation classification tasks. The key training details are:\nFor pre-training, the model was trained using the masked language modeling (MLM) objective, where a random subset of the input tokens are masked and the model learns to predict the original tokens based on the surrounding context. This allows the model to learn bidirectional representations that capture the semantic relationships between words.\nThe pre-training was done for 1 million steps with a batch size of 256 sequences, using the Adam optimizer with a learning rate of 1e-4. The model was trained on 8 NVIDIA V100 GPUs for approximately 2 weeks.\nFor fine-tuning, the pre-trained model was further trained on the entity and relation classification tasks using the labeled datasets. The entity classification head was trained using the cross-entropy loss between the predicted and true entity labels, while the relation classification head was trained using the binary cross-entropy loss between the predicted and true relation labels.\nThe fine-tuning was done for 10 epochs with a batch size of 32 sequences, using the Adam optimizer with a learning rate of 5e-5. The model was fine-tuned on a single NVIDIA V100 GPU for approximately 1 day.\nHyperparameter tuning was performed using grid search over the learning rate, batch size, and number of fine-tuning epochs, based on the performance on the validation set.\nThe two-stage training process allows BiomedParse to leverage the large unlabeled biomedical corpus for learning general-purpose representations, while still being able to adapt to the specific entity and relation classification tasks using the labeled datasets.\n4. Model Performance and Evaluation BiomedParse was evaluated on several benchmark datasets for biomedical named entity recognition (NER) and relation extraction (RE), including:\nBC5CDR (NER and RE for chemicals and diseases) NCBI-Disease (NER for diseases) BC2GM (NER for genes and proteins) BioCreative VI ChemProt (RE for chemical-protein interactions) The key evaluation metrics were:\nPrecision: the fraction of predicted entities/relations that are correct Recall: the fraction of true entities/relations that are predicted F1 score: the harmonic mean of precision and recall BiomedParse achieved state-of-the-art performance on all benchmark datasets, with F1 scores ranging from 87.2% to 91.5% for NER and 76.3% to 83.1% for RE. This represents an improvement of 2-5 percentage points over previous methods based on BiLSTM-CRF (Bidirectional Long Short-Term Memory with Conditional Random Field) and rule-based systems.\nAblation studies showed that both the domain-specific pre-training and multi-task learning contributed significantly to the model’s performance, with pre-training providing a larger boost than multi-task learning. The model’s performance was also robust to the choice of hyperparameters, with similar results obtained across a range of learning rates and batch sizes.\n5. Biological Relevance and Interpretation BiomedParse learns biologically meaningful representations of biomedical entities and their relationships, as evidenced by its ability to accurately extract them from raw text. Some key observations are:\nThe model can recognize a wide range of biomedical entities, including genes, proteins, drugs, chemicals, and diseases, based on their textual descriptions. This suggests that the model has learned the relevant biological concepts and terminology.\nThe model can identify complex relationships between entities, such as gene-disease associations, drug-drug interactions, and chemical-protein interactions. This demonstrates the model’s ability to capture the underlying biological mechanisms and pathways.\nThe model’s predictions are largely consistent with existing biomedical knowledge bases, such as UniProt, DrugBank, and OMIM, which were used to create the training datasets. This provides external validation of the model’s outputs.\nThe attention weights of the transformer architecture can be visualized to interpret the model’s predictions and identify the key words and phrases that contribute to each entity/relation. This can potentially provide new insights into the biological basis of the model’s decisions.\nHowever, as with all machine learning models, BiomedParse’s predictions should be interpreted with caution and validated by expert human reviewers. The model may make errors or learn spurious correlations from the training data, and its outputs should be treated as hypotheses rather than definitive facts.\n6. Computational Efficiency BiomedParse is computationally efficient compared to traditional rule-based and feature-engineering approaches for biomedical text mining, thanks to the parallelizable architecture of the transformer model:\nThe pre-training and fine-tuning can be efficiently distributed across multiple GPUs, allowing the model to be trained on large datasets in a reasonable amount of time (e.g., 2 weeks for pre-training and 1 day for fine-tuning).\nThe inference time is fast, taking only a few seconds per abstract on a single GPU. This enables the model to be applied to large-scale literature mining tasks, such as extracting entities and relations from the entire PubMed database.\nThe model’s memory footprint is relatively small, requiring only a few gigabytes of GPU memory for inference. This makes it possible to deploy the model on standard hardware or cloud computing platforms.\nHowever, the computational cost of training the model from scratch is still significant, requiring access to high-performance computing resources. Fine-tuning the model on new datasets or tasks is more affordable, but still requires some computational overhead.\n7. Challenges and Limitations Despite its strong performance and efficiency, BiomedParse has several challenges and limitations that need to be addressed in future work:\nThe model’s performance depends on the quality and coverage of the training datasets, which may not be representative of all biomedical subdomains or text genres. Expanding the training data to include more diverse sources, such as clinical notes, patents, and social media, could improve the model’s generalizability.\nThe model’s outputs are limited to the entity and relation types that are defined in the training datasets, which may not capture all the relevant information in the text. Developing methods for open-ended information extraction, such as event extraction or knowledge graph construction, could provide a more comprehensive representation of the biomedical knowledge.\nThe model’s interpretability is limited by the complexity of the transformer architecture and the lack of explicit symbolic representations. Developing more transparent and explainable models, such as rule-based systems or knowledge-infused neural networks, could improve the trustworthiness and acceptability of the model’s predictions.\nThe model’s ability to handle complex linguistic phenomena, such as negation, coreference, and hedging, is limited by the simplicity of the entity and relation classification tasks. Incorporating more sophisticated natural language understanding techniques, such as semantic parsing or discourse analysis, could improve the model’s ability to extract accurate and nuanced information from the text.\nAddressing these challenges will require a combination of technical innovations, such as new model architectures and training paradigms, as well as interdisciplinary collaborations between computer scientists, biomedical experts, and end users.\n8. Future Directions and Improvements There are many promising directions for extending and improving the BiomedParse model, including:\nExpanding the model to handle more diverse biomedical text genres, such as clinical notes, social media, and scientific figures, which may require different preprocessing, tokenization, and modeling techniques.\nDeveloping methods for open-ended information extraction, such as event extraction, knowledge graph construction, and summarization, which can provide a more comprehensive and structured representation of the biomedical knowledge.\nIncorporating domain knowledge and symbolic reasoning into the model, such as ontologies, rules, and constraints, which can improve the interpretability and robustness of the model’s predictions.\nExploring new model architectures and training paradigms, such as graph neural networks, adversarial learning, and reinforcement learning, which can capture more complex linguistic and semantic relationships in the text.\nIntegrating the model with other biomedical data sources and tools, such as databases, ontologies, and visualization platforms, to enable end-to-end knowledge discovery and hypothesis generation.\nDeveloping user-friendly interfaces and workflows for biomedical researchers and clinicians to interact with the model and incorporate its outputs into their decision-making processes.\nUltimately, the goal is to develop BiomedParse into a powerful and reliable tool for extracting actionable insights from the vast amount of biomedical literature, and accelerating the pace of scientific discovery and clinical translation.\n9. Conclusion BiomedParse is a state-of-the-art transformer-based language model for biomedical text parsing, which achieves strong performance on named entity recognition and relation extraction tasks across multiple benchmark datasets. By leveraging domain-specific pre-training and multi-task learning, BiomedParse can learn biologically meaningful representations of biomedical entities and their relationships, and efficiently extract them from raw text.\nThe key strengths of BiomedParse are its ability to handle the complexity and diversity of biomedical language, its computational efficiency and scalability, and its potential for enabling large-scale literature mining and knowledge discovery. These strengths make BiomedParse a promising tool for accelerating biomedical research and clinical practice, by providing a more efficient and comprehensive way to access and integrate the knowledge contained in the biomedical literature.\nHowever, BiomedParse also has important limitations and challenges, such as its dependence on the quality and coverage of the training data, its limited interpretability and explainability, and its inability to handle more complex linguistic and semantic phenomena. Addressing these limitations will require ongoing research and development efforts, as well as collaborations between the natural language processing and biomedical communities.\nDespite these challenges, BiomedParse represents an exciting direction for advancing biomedical text mining and knowledge discovery, by leveraging the power of deep learning and natural language processing. As the model continues to evolve and improve, it has the potential to become a key enabling technology for data-driven biomedical research and precision medicine, helping to unlock the full value of the biomedical literature and accelerate the translation of scientific discoveries into clinical applications.\n","wordCount":"2048","inLanguage":"en","image":"https://dada325.github.io/dadaBlog/%3Cimage%20path/url%3E","datePublished":"2024-06-06T12:54:20+08:00","dateModified":"2024-06-06T12:54:20+08:00","author":{"@type":"Person","name":"dada"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://dada325.github.io/dadaBlog/posts/bioparse-review/"},"publisher":{"@type":"Organization","name":"Math, Bio, Programming","logo":{"@type":"ImageObject","url":"https://dada325.github.io/dadaBlog/favicon.ico"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://dada325.github.io/dadaBlog/ accesskey=h title="DasBlog (Alt + H)"><img src=https://dada325.github.io/apple-touch-icon.png alt aria-label=logo height=35>DasBlog</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)">
<svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg>
<svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://dada325.github.io/dadaBlog/posts/ title=Posts><span>Posts</span></a></li><li><a href=https://dada325.github.io/dadaBlog/archive/ title=Archive><span>Archive</span></a></li><li><a href=https://dada325.github.io/dadaBlog/search/ title="Search (Alt + /)" accesskey=/><span>Search</span></a></li><li><a href=https://dada325.github.io/dadaBlog/tags/ title=Tags><span>Tags</span></a></li><li><a href=https://dada325.github.io/dadaBlog/faq/ title=FAQ><span>FAQ</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://dada325.github.io/dadaBlog/>Home</a>&nbsp;»&nbsp;<a href=https://dada325.github.io/dadaBlog/posts/>Posts</a></div><h1 class="post-title entry-hint-parent">BiomedParse Review</h1><div class=post-meta><span title='2024-06-06 12:54:20 +0800 +0800'>June 6, 2024</span>&nbsp;·&nbsp;10 min&nbsp;·&nbsp;2048 words&nbsp;·&nbsp;dada&nbsp;|&nbsp;<a href=https://github.com/%3cpath_to_repo%3e/content/posts/BioParse-review.md rel="noopener noreferrer" target=_blank>Suggest Changes</a></div></header><div class=toc><details><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><nav id=TableOfContents><ul><li><a href=#biomedparse>BiomedParse</a></li><li><a href=#1-model-selection-and-architecture>1. Model Selection and Architecture</a></li><li><a href=#2-data-handling-and-preprocessing>2. Data Handling and Preprocessing</a></li><li><a href=#3-training-process>3. Training Process</a></li><li><a href=#4-model-performance-and-evaluation>4. Model Performance and Evaluation</a></li><li><a href=#5-biological-relevance-and-interpretation>5. Biological Relevance and Interpretation</a></li><li><a href=#6-computational-efficiency>6. Computational Efficiency</a></li><li><a href=#7-challenges-and-limitations>7. Challenges and Limitations</a></li><li><a href=#8-future-directions-and-improvements>8. Future Directions and Improvements</a></li><li><a href=#9-conclusion>9. Conclusion</a></li></ul></nav></div></details></div><div class=post-content><h2 id=biomedparse>BiomedParse<a hidden class=anchor aria-hidden=true href=#biomedparse>#</a></h2><h2 id=1-model-selection-and-architecture>1. Model Selection and Architecture<a hidden class=anchor aria-hidden=true href=#1-model-selection-and-architecture>#</a></h2><p>BiomedParse is a transformer-based language model specifically designed for parsing biomedical text into structured representations. The key architectural choices are:</p><ul><li><p>Using a pre-trained BERT (Bidirectional Encoder Representations from Transformers) model as the backbone, which has shown strong performance on various natural language processing tasks. BERT is a deep bidirectional transformer that learns contextual word representations by jointly conditioning on both left and right context.</p></li><li><p>Fine-tuning the pre-trained BERT model on a large corpus of biomedical text (PubMed abstracts and PMC full-text articles) to adapt it to the biomedical domain. This domain-specific pre-training allows the model to learn the unique vocabulary, grammar, and semantic relationships in biomedical literature.</p></li><li><p>Adding a token classification head on top of the BERT encoder to predict the entity labels (e.g., gene, disease, drug) for each input token. The token classification head is a linear layer followed by a softmax activation, which outputs the probability distribution over the entity labels.</p></li><li><p>Adding a relation classification head to predict the relations (e.g., gene-disease association, drug-drug interaction) between pairs of entities. The relation classification head takes the concatenated entity representations as input and outputs the probability distribution over the relation types.</p></li><li><p>Jointly training the entity and relation classification heads using a multi-task learning objective, which combines the cross-entropy losses for both tasks. This joint training allows the model to learn the interactions between entities and relations and improve the overall parsing performance.</p></li></ul><p>The transformer architecture allows BiomedParse to capture long-range dependencies and complex semantic relationships in biomedical text, while the domain-specific pre-training and multi-task learning enable it to adapt to the unique characteristics of biomedical literature.</p><h2 id=2-data-handling-and-preprocessing>2. Data Handling and Preprocessing<a hidden class=anchor aria-hidden=true href=#2-data-handling-and-preprocessing>#</a></h2><p>BiomedParse was trained on a large corpus of biomedical text, including:</p><ul><li>14 million PubMed abstracts (3.2 billion words)</li><li>2.7 million PMC full-text articles (7.5 billion words)</li></ul><p>The key preprocessing steps were:</p><ul><li><p>Tokenizing the text into subword units using the WordPiece tokenizer, which splits words into smaller units based on their frequency in the corpus. This allows the model to handle out-of-vocabulary words and capture morphological patterns.</p></li><li><p>Representing each input sequence as a sequence of token IDs, along with special tokens like [CLS] (classification token) and [SEP] (separator token) to demarcate the sequence boundaries.</p></li><li><p>Creating entity labels for each token using the BIO (Beginning-Inside-Outside) tagging scheme, where &lsquo;B&rsquo; denotes the beginning of an entity, &lsquo;I&rsquo; denotes the inside of an entity, and &lsquo;O&rsquo; denotes outside of an entity. The entity labels were obtained from existing biomedical named entity recognition datasets.</p></li><li><p>Creating relation labels for each pair of entities in the input sequence, based on existing biomedical relation extraction datasets. The relation labels were represented as a binary matrix, where each cell indicates the presence or absence of a relation between two entities.</p></li></ul><p>The preprocessed data was split into training, validation, and test sets for model development and evaluation. No data augmentation was used, as the large size of the training corpus was deemed sufficient for learning robust representations.</p><h2 id=3-training-process>3. Training Process<a hidden class=anchor aria-hidden=true href=#3-training-process>#</a></h2><p>BiomedParse was trained using a two-stage process: pre-training on the biomedical corpus, followed by fine-tuning on the entity and relation classification tasks. The key training details are:</p><ul><li><p>For pre-training, the model was trained using the masked language modeling (MLM) objective, where a random subset of the input tokens are masked and the model learns to predict the original tokens based on the surrounding context. This allows the model to learn bidirectional representations that capture the semantic relationships between words.</p></li><li><p>The pre-training was done for 1 million steps with a batch size of 256 sequences, using the Adam optimizer with a learning rate of 1e-4. The model was trained on 8 NVIDIA V100 GPUs for approximately 2 weeks.</p></li><li><p>For fine-tuning, the pre-trained model was further trained on the entity and relation classification tasks using the labeled datasets. The entity classification head was trained using the cross-entropy loss between the predicted and true entity labels, while the relation classification head was trained using the binary cross-entropy loss between the predicted and true relation labels.</p></li><li><p>The fine-tuning was done for 10 epochs with a batch size of 32 sequences, using the Adam optimizer with a learning rate of 5e-5. The model was fine-tuned on a single NVIDIA V100 GPU for approximately 1 day.</p></li><li><p>Hyperparameter tuning was performed using grid search over the learning rate, batch size, and number of fine-tuning epochs, based on the performance on the validation set.</p></li></ul><p>The two-stage training process allows BiomedParse to leverage the large unlabeled biomedical corpus for learning general-purpose representations, while still being able to adapt to the specific entity and relation classification tasks using the labeled datasets.</p><h2 id=4-model-performance-and-evaluation>4. Model Performance and Evaluation<a hidden class=anchor aria-hidden=true href=#4-model-performance-and-evaluation>#</a></h2><p>BiomedParse was evaluated on several benchmark datasets for biomedical named entity recognition (NER) and relation extraction (RE), including:</p><ul><li>BC5CDR (NER and RE for chemicals and diseases)</li><li>NCBI-Disease (NER for diseases)</li><li>BC2GM (NER for genes and proteins)</li><li>BioCreative VI ChemProt (RE for chemical-protein interactions)</li></ul><p>The key evaluation metrics were:</p><ul><li>Precision: the fraction of predicted entities/relations that are correct</li><li>Recall: the fraction of true entities/relations that are predicted</li><li>F1 score: the harmonic mean of precision and recall</li></ul><p>BiomedParse achieved state-of-the-art performance on all benchmark datasets, with F1 scores ranging from 87.2% to 91.5% for NER and 76.3% to 83.1% for RE. This represents an improvement of 2-5 percentage points over previous methods based on BiLSTM-CRF (Bidirectional Long Short-Term Memory with Conditional Random Field) and rule-based systems.</p><p>Ablation studies showed that both the domain-specific pre-training and multi-task learning contributed significantly to the model&rsquo;s performance, with pre-training providing a larger boost than multi-task learning. The model&rsquo;s performance was also robust to the choice of hyperparameters, with similar results obtained across a range of learning rates and batch sizes.</p><h2 id=5-biological-relevance-and-interpretation>5. Biological Relevance and Interpretation<a hidden class=anchor aria-hidden=true href=#5-biological-relevance-and-interpretation>#</a></h2><p>BiomedParse learns biologically meaningful representations of biomedical entities and their relationships, as evidenced by its ability to accurately extract them from raw text. Some key observations are:</p><ul><li><p>The model can recognize a wide range of biomedical entities, including genes, proteins, drugs, chemicals, and diseases, based on their textual descriptions. This suggests that the model has learned the relevant biological concepts and terminology.</p></li><li><p>The model can identify complex relationships between entities, such as gene-disease associations, drug-drug interactions, and chemical-protein interactions. This demonstrates the model&rsquo;s ability to capture the underlying biological mechanisms and pathways.</p></li><li><p>The model&rsquo;s predictions are largely consistent with existing biomedical knowledge bases, such as UniProt, DrugBank, and OMIM, which were used to create the training datasets. This provides external validation of the model&rsquo;s outputs.</p></li><li><p>The attention weights of the transformer architecture can be visualized to interpret the model&rsquo;s predictions and identify the key words and phrases that contribute to each entity/relation. This can potentially provide new insights into the biological basis of the model&rsquo;s decisions.</p></li></ul><p>However, as with all machine learning models, BiomedParse&rsquo;s predictions should be interpreted with caution and validated by expert human reviewers. The model may make errors or learn spurious correlations from the training data, and its outputs should be treated as hypotheses rather than definitive facts.</p><h2 id=6-computational-efficiency>6. Computational Efficiency<a hidden class=anchor aria-hidden=true href=#6-computational-efficiency>#</a></h2><p>BiomedParse is computationally efficient compared to traditional rule-based and feature-engineering approaches for biomedical text mining, thanks to the parallelizable architecture of the transformer model:</p><ul><li><p>The pre-training and fine-tuning can be efficiently distributed across multiple GPUs, allowing the model to be trained on large datasets in a reasonable amount of time (e.g., 2 weeks for pre-training and 1 day for fine-tuning).</p></li><li><p>The inference time is fast, taking only a few seconds per abstract on a single GPU. This enables the model to be applied to large-scale literature mining tasks, such as extracting entities and relations from the entire PubMed database.</p></li><li><p>The model&rsquo;s memory footprint is relatively small, requiring only a few gigabytes of GPU memory for inference. This makes it possible to deploy the model on standard hardware or cloud computing platforms.</p></li></ul><p>However, the computational cost of training the model from scratch is still significant, requiring access to high-performance computing resources. Fine-tuning the model on new datasets or tasks is more affordable, but still requires some computational overhead.</p><h2 id=7-challenges-and-limitations>7. Challenges and Limitations<a hidden class=anchor aria-hidden=true href=#7-challenges-and-limitations>#</a></h2><p>Despite its strong performance and efficiency, BiomedParse has several challenges and limitations that need to be addressed in future work:</p><ul><li><p>The model&rsquo;s performance depends on the quality and coverage of the training datasets, which may not be representative of all biomedical subdomains or text genres. Expanding the training data to include more diverse sources, such as clinical notes, patents, and social media, could improve the model&rsquo;s generalizability.</p></li><li><p>The model&rsquo;s outputs are limited to the entity and relation types that are defined in the training datasets, which may not capture all the relevant information in the text. Developing methods for open-ended information extraction, such as event extraction or knowledge graph construction, could provide a more comprehensive representation of the biomedical knowledge.</p></li><li><p>The model&rsquo;s interpretability is limited by the complexity of the transformer architecture and the lack of explicit symbolic representations. Developing more transparent and explainable models, such as rule-based systems or knowledge-infused neural networks, could improve the trustworthiness and acceptability of the model&rsquo;s predictions.</p></li><li><p>The model&rsquo;s ability to handle complex linguistic phenomena, such as negation, coreference, and hedging, is limited by the simplicity of the entity and relation classification tasks. Incorporating more sophisticated natural language understanding techniques, such as semantic parsing or discourse analysis, could improve the model&rsquo;s ability to extract accurate and nuanced information from the text.</p></li></ul><p>Addressing these challenges will require a combination of technical innovations, such as new model architectures and training paradigms, as well as interdisciplinary collaborations between computer scientists, biomedical experts, and end users.</p><h2 id=8-future-directions-and-improvements>8. Future Directions and Improvements<a hidden class=anchor aria-hidden=true href=#8-future-directions-and-improvements>#</a></h2><p>There are many promising directions for extending and improving the BiomedParse model, including:</p><ul><li><p>Expanding the model to handle more diverse biomedical text genres, such as clinical notes, social media, and scientific figures, which may require different preprocessing, tokenization, and modeling techniques.</p></li><li><p>Developing methods for open-ended information extraction, such as event extraction, knowledge graph construction, and summarization, which can provide a more comprehensive and structured representation of the biomedical knowledge.</p></li><li><p>Incorporating domain knowledge and symbolic reasoning into the model, such as ontologies, rules, and constraints, which can improve the interpretability and robustness of the model&rsquo;s predictions.</p></li><li><p>Exploring new model architectures and training paradigms, such as graph neural networks, adversarial learning, and reinforcement learning, which can capture more complex linguistic and semantic relationships in the text.</p></li><li><p>Integrating the model with other biomedical data sources and tools, such as databases, ontologies, and visualization platforms, to enable end-to-end knowledge discovery and hypothesis generation.</p></li><li><p>Developing user-friendly interfaces and workflows for biomedical researchers and clinicians to interact with the model and incorporate its outputs into their decision-making processes.</p></li></ul><p>Ultimately, the goal is to develop BiomedParse into a powerful and reliable tool for extracting actionable insights from the vast amount of biomedical literature, and accelerating the pace of scientific discovery and clinical translation.</p><h2 id=9-conclusion>9. Conclusion<a hidden class=anchor aria-hidden=true href=#9-conclusion>#</a></h2><p>BiomedParse is a state-of-the-art transformer-based language model for biomedical text parsing, which achieves strong performance on named entity recognition and relation extraction tasks across multiple benchmark datasets. By leveraging domain-specific pre-training and multi-task learning, BiomedParse can learn biologically meaningful representations of biomedical entities and their relationships, and efficiently extract them from raw text.</p><p>The key strengths of BiomedParse are its ability to handle the complexity and diversity of biomedical language, its computational efficiency and scalability, and its potential for enabling large-scale literature mining and knowledge discovery. These strengths make BiomedParse a promising tool for accelerating biomedical research and clinical practice, by providing a more efficient and comprehensive way to access and integrate the knowledge contained in the biomedical literature.</p><p>However, BiomedParse also has important limitations and challenges, such as its dependence on the quality and coverage of the training data, its limited interpretability and explainability, and its inability to handle more complex linguistic and semantic phenomena. Addressing these limitations will require ongoing research and development efforts, as well as collaborations between the natural language processing and biomedical communities.</p><p>Despite these challenges, BiomedParse represents an exciting direction for advancing biomedical text mining and knowledge discovery, by leveraging the power of deep learning and natural language processing. As the model continues to evolve and improve, it has the potential to become a key enabling technology for data-driven biomedical research and precision medicine, helping to unlock the full value of the biomedical literature and accelerate the translation of scientific discoveries into clinical applications.</p></div><footer class=post-footer><ul class=post-tags><li><a href=https://dada325.github.io/dadaBlog/tags/llm/>LLM</a></li></ul><nav class=paginav><a class=prev href=https://dada325.github.io/dadaBlog/posts/contextual-ai-models-for-single-cell-protein-biology/><span class=title>« Prev</span><br><span>Contextual AI Models for Single Cell Protein Biology</span>
</a><a class=next href=https://dada325.github.io/dadaBlog/posts/cancer-genomes-3d/><span class=title>Next »</span><br><span>Cancer Genomes 3D</span></a></nav><ul class=share-buttons><li><a target=_blank rel="noopener noreferrer" aria-label="share BiomedParse Review on x" href="https://x.com/intent/tweet/?text=BiomedParse%20Review&amp;url=https%3a%2f%2fdada325.github.io%2fdadaBlog%2fposts%2fbioparse-review%2f&amp;hashtags=LLM"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M512 62.554V449.446C512 483.97 483.97 512 449.446 512H62.554C28.03 512 0 483.97.0 449.446V62.554C0 28.03 28.029.0 62.554.0H449.446C483.971.0 512 28.03 512 62.554zM269.951 190.75 182.567 75.216H56L207.216 272.95 63.9 436.783h61.366L235.9 310.383l96.667 126.4H456L298.367 228.367l134-153.151H371.033zM127.633 110h36.468l219.38 290.065H349.5z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share BiomedParse Review on linkedin" href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https%3a%2f%2fdada325.github.io%2fdadaBlog%2fposts%2fbioparse-review%2f&amp;title=BiomedParse%20Review&amp;summary=BiomedParse%20Review&amp;source=https%3a%2f%2fdada325.github.io%2fdadaBlog%2fposts%2fbioparse-review%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM160.461 423.278V197.561h-75.04v225.717h75.04zm270.539.0V293.839c0-69.333-37.018-101.586-86.381-101.586-39.804.0-57.634 21.891-67.617 37.266v-31.958h-75.021c.995 21.181.0 225.717.0 225.717h75.02V297.222c0-6.748.486-13.492 2.474-18.315 5.414-13.475 17.767-27.434 38.494-27.434 27.135.0 38.007 20.707 38.007 51.037v120.768H431zM123.448 88.722C97.774 88.722 81 105.601 81 127.724c0 21.658 16.264 39.002 41.455 39.002h.484c26.165.0 42.452-17.344 42.452-39.002-.485-22.092-16.241-38.954-41.943-39.002z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share BiomedParse Review on reddit" href="https://reddit.com/submit?url=https%3a%2f%2fdada325.github.io%2fdadaBlog%2fposts%2fbioparse-review%2f&title=BiomedParse%20Review"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM446 265.638c0-22.964-18.616-41.58-41.58-41.58-11.211.0-21.361 4.457-28.841 11.666-28.424-20.508-67.586-33.757-111.204-35.278l18.941-89.121 61.884 13.157c.756 15.734 13.642 28.29 29.56 28.29 16.407.0 29.706-13.299 29.706-29.701.0-16.403-13.299-29.702-29.706-29.702-11.666.0-21.657 6.792-26.515 16.578l-69.105-14.69c-1.922-.418-3.939-.042-5.585 1.036-1.658 1.073-2.811 2.761-3.224 4.686l-21.152 99.438c-44.258 1.228-84.046 14.494-112.837 35.232-7.468-7.164-17.589-11.591-28.757-11.591-22.965.0-41.585 18.616-41.585 41.58.0 16.896 10.095 31.41 24.568 37.918-.639 4.135-.99 8.328-.99 12.576.0 63.977 74.469 115.836 166.33 115.836s166.334-51.859 166.334-115.836c0-4.218-.347-8.387-.977-12.493 14.564-6.47 24.735-21.034 24.735-38.001zM326.526 373.831c-20.27 20.241-59.115 21.816-70.534 21.816-11.428.0-50.277-1.575-70.522-21.82-3.007-3.008-3.007-7.882.0-10.889 3.003-2.999 7.882-3.003 10.885.0 12.777 12.781 40.11 17.317 59.637 17.317 19.522.0 46.86-4.536 59.657-17.321 3.016-2.999 7.886-2.995 10.885.008 3.008 3.011 3.003 7.882-.008 10.889zm-5.23-48.781c-16.373.0-29.701-13.324-29.701-29.698.0-16.381 13.328-29.714 29.701-29.714 16.378.0 29.706 13.333 29.706 29.714.0 16.374-13.328 29.698-29.706 29.698zM160.91 295.348c0-16.381 13.328-29.71 29.714-29.71 16.369.0 29.689 13.329 29.689 29.71.0 16.373-13.32 29.693-29.689 29.693-16.386.0-29.714-13.32-29.714-29.693z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share BiomedParse Review on facebook" href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2fdada325.github.io%2fdadaBlog%2fposts%2fbioparse-review%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H342.978V319.085h66.6l12.672-82.621h-79.272v-53.617c0-22.603 11.073-44.636 46.58-44.636H425.6v-70.34s-32.71-5.582-63.982-5.582c-65.288.0-107.96 39.569-107.96 111.204v62.971h-72.573v82.621h72.573V512h-191.104c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share BiomedParse Review on whatsapp" href="https://api.whatsapp.com/send?text=BiomedParse%20Review%20-%20https%3a%2f%2fdada325.github.io%2fdadaBlog%2fposts%2fbioparse-review%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zm-58.673 127.703c-33.842-33.881-78.847-52.548-126.798-52.568-98.799.0-179.21 80.405-179.249 179.234-.013 31.593 8.241 62.428 23.927 89.612l-25.429 92.884 95.021-24.925c26.181 14.28 55.659 21.807 85.658 21.816h.074c98.789.0 179.206-80.413 179.247-179.243.018-47.895-18.61-92.93-52.451-126.81zM263.976 403.485h-.06c-26.734-.01-52.954-7.193-75.828-20.767l-5.441-3.229-56.386 14.792 15.05-54.977-3.542-5.637c-14.913-23.72-22.791-51.136-22.779-79.287.033-82.142 66.867-148.971 149.046-148.971 39.793.014 77.199 15.531 105.329 43.692 28.128 28.16 43.609 65.592 43.594 105.4-.034 82.149-66.866 148.983-148.983 148.984zm81.721-111.581c-4.479-2.242-26.499-13.075-30.604-14.571-4.105-1.495-7.091-2.241-10.077 2.241-2.986 4.483-11.569 14.572-14.182 17.562-2.612 2.988-5.225 3.364-9.703 1.12-4.479-2.241-18.91-6.97-36.017-22.23C231.8 264.15 222.81 249.484 220.198 245s-.279-6.908 1.963-9.14c2.016-2.007 4.48-5.232 6.719-7.847 2.24-2.615 2.986-4.484 4.479-7.472 1.493-2.99.747-5.604-.374-7.846-1.119-2.241-10.077-24.288-13.809-33.256-3.635-8.733-7.327-7.55-10.077-7.688-2.609-.13-5.598-.158-8.583-.158-2.986.0-7.839 1.121-11.944 5.604-4.105 4.484-15.675 15.32-15.675 37.364.0 22.046 16.048 43.342 18.287 46.332 2.24 2.99 31.582 48.227 76.511 67.627 10.685 4.615 19.028 7.371 25.533 9.434 10.728 3.41 20.492 2.929 28.209 1.775 8.605-1.285 26.499-10.833 30.231-21.295 3.732-10.464 3.732-19.431 2.612-21.298-1.119-1.869-4.105-2.99-8.583-5.232z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share BiomedParse Review on telegram" href="https://telegram.me/share/url?text=BiomedParse%20Review&amp;url=https%3a%2f%2fdada325.github.io%2fdadaBlog%2fposts%2fbioparse-review%2f"><svg viewBox="2 2 28 28" height="30" width="30" fill="currentcolor"><path d="M26.49 29.86H5.5a3.37 3.37.0 01-2.47-1 3.35 3.35.0 01-1-2.47V5.48A3.36 3.36.0 013 3 3.37 3.37.0 015.5 2h21A3.38 3.38.0 0129 3a3.36 3.36.0 011 2.46V26.37a3.35 3.35.0 01-1 2.47 3.38 3.38.0 01-2.51 1.02zm-5.38-6.71a.79.79.0 00.85-.66L24.73 9.24a.55.55.0 00-.18-.46.62.62.0 00-.41-.17q-.08.0-16.53 6.11a.59.59.0 00-.41.59.57.57.0 00.43.52l4 1.24 1.61 4.83a.62.62.0 00.63.43.56.56.0 00.4-.17L16.54 20l4.09 3A.9.9.0 0021.11 23.15zM13.8 20.71l-1.21-4q8.72-5.55 8.78-5.55c.15.0.23.0.23.16a.18.18.0 010 .06s-2.51 2.3-7.52 6.8z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share BiomedParse Review on ycombinator" href="https://news.ycombinator.com/submitlink?t=BiomedParse%20Review&u=https%3a%2f%2fdada325.github.io%2fdadaBlog%2fposts%2fbioparse-review%2f"><svg width="30" height="30" viewBox="0 0 512 512" fill="currentcolor" xmlns:inkscape="http://www.inkscape.org/namespaces/inkscape"><path d="M449.446.0C483.971.0 512 28.03 512 62.554V449.446C512 483.97 483.97 512 449.446 512H62.554C28.03 512 0 483.97.0 449.446V62.554C0 28.03 28.029.0 62.554.0H449.446zM183.8767 87.9921h-62.034L230.6673 292.4508V424.0079h50.6655V292.4508L390.1575 87.9921H328.1233L256 238.2489z"/></svg></a></li></ul></footer></article></main><footer class=footer><span>&copy; 2024 <a href=https://dada325.github.io/dadaBlog/>Math, Bio, Programming</a></span> ·
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script></body></html>